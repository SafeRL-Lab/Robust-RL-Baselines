# Robust-RL-Benchmark



#### 1. Robust Single Agent RL Baselines

- Robust offline Reinforcement Learning via Conservative Smoothing, [Paper](https://arxiv.org/pdf/2206.02829), [Code](https://github.com/YangRui2015/RORL), (Accepted by NeurIPS 2022)
- Robust Risk-Aware Reinforcement Learning, [Paper](https://arxiv.org/pdf/2108.10403), [Code](https://github.com/sebjai/robust-risk-aware-rl), (Accepted by SIAM Journal on Financial Mathematics, 2022)
- Robust Reinforcement Learning with Alternating Training of Learned Adversaries,  [Paper](https://arxiv.org/pdf/2101.08452), [Code](https://github.com/huanzhang12/ATLA_robust_RL), (Accepted by ICLR 2021)
- Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations, [Paper](https://arxiv.org/pdf/2003.08938), [Code](https://github.com/chenhongge/StateAdvDRL), (Accepted by NeurIPS 2020)
- Action Robust Reinforcement Learning and Applications in Continuous Control, [Paper](https://arxiv.org/pdf/1901.09184), [Code](https://github.com/tesslerc/ActionRobustRL), (Accepted by ICML 2019)
- Robust Domain Randomization for Reinforcement Learning, [Paper](https://arxiv.org/pdf/1910.10537), [Code](https://github.com/uncharted-technologies/robust-domain-randomization), (Arxiv, 2019)
- Robust Adversarial Reinforcement Learning, [Paper](https://arxiv.org/pdf/1703.02702), [Code](https://github.com/jerinphilip/robust-adversarial-rl), (Accepted by ICML 2015)



#### 2. Robust Multi-Agent RL Baselines

